# ğŸ§  BERT Fine-Tuning for AG News Classification

This project fine-tunes a pre-trained BERT model on the [AG News dataset](https://huggingface.co/datasets/ag_news) to perform topic classification. It was built as part of a self-study plan to deepen understanding of transformer-based NLP pipelines, PyTorch workflows, and efficient dataset handling.

---

## ğŸš€ Overview

- Fine-tuned a pre-trained BERT (and DistilBERT) model for 4-class news topic classification.
- Implemented data preprocessing, model training, evaluation, and metric visualization from scratch.
- Optimized data loading by caching tokenized datasets and avoiding redundant computation.

---

## ğŸ—‚ Project Structure

BertFineTuneClassification/

â”œâ”€â”€ data/ # CSV file and tokenized datasets

â”‚ â””â”€â”€ tokenized_*.pt

â”œâ”€â”€ src/

â”‚ â”œâ”€â”€ dataset.py # Dataset loading & tokenization (cached)

â”‚ â”œâ”€â”€ evaluation.py # Test set evaluation logic

â”‚ â”œâ”€â”€ train.py # Training loop with metric tracking

â”‚ â””â”€â”€ visualization.py # Metric plotting functions

â”œâ”€â”€ training_plots/ # Output accuracy/loss plots

â”œâ”€â”€ model.py # Model loading logic (DistilBERT)

â”œâ”€â”€ run.py # Entry point: loads data, trains, evaluates

â”œâ”€â”€ report.md  # Summary of work and lessons learned.

â””â”€â”€ README.md

## âš™ï¸ Usage
Train the model:

python run.py

Tokenization:

Tokenized versions of train, val, and test sets are cached in the data/ directory.

If cached files exist, they will be loaded directly to skip tokenization.

Model Checkpoint:

The best model based on validation loss is saved in models/best_model.pt (not shown in repo structure until generated).

Plotting:

Accuracy and loss plots are saved in training_plots/.

Evaluation:

After training, the best model is evaluated on the test set.

Results (average loss and accuracy) are logged.
